{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, minmax_scale\n",
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: \n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "# Seen from https://www.kaggle.com/kabure/almost-complete-feature-engineering-ieee-data#V-Features\n",
    "def PCA_change(df, cols, n_components, prefix='PCA_', rand_seed=4):\n",
    "    pca = PCA(n_components=n_components, random_state=rand_seed)\n",
    "\n",
    "    principalComponents = pca.fit_transform(df[cols])\n",
    "\n",
    "    principalDf = pd.DataFrame(principalComponents)\n",
    "\n",
    "    df.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "    principalDf.rename(columns=lambda x: str(prefix)+str(x), inplace=True)\n",
    "\n",
    "    df = pd.concat([df, principalDf], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def resumetable(df):\n",
    "    print(f\"Dataset Shape: {df.shape}\")\n",
    "    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n",
    "    summary = summary.reset_index()\n",
    "    summary['Name'] = summary['index']\n",
    "    summary = summary[['Name','dtypes']]\n",
    "    summary['Missing'] = df.isnull().sum().values    \n",
    "    summary['Uniques'] = df.nunique().values\n",
    "    summary['First Value'] = df.loc[0].values\n",
    "    summary['Second Value'] = df.loc[1].values\n",
    "    summary['Third Value'] = df.loc[2].values\n",
    "\n",
    "    for name in summary['Name'].value_counts().index:\n",
    "        # print(name)\n",
    "        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_identity_train = pd.read_csv(\"../Data/train_identity.csv\")\n",
    "df_transaction_train = pd.read_csv(\"../Data/train_transaction.csv\")\n",
    "df_identity_test = pd.read_csv(\"../Data/test_identity.csv\")\n",
    "df_transaction_test = pd.read_csv(\"../Data/test_transaction.csv\")\n",
    "df_train = pd.merge(df_transaction_train, df_identity_train, how = \"left\", on = \"TransactionID\")\n",
    "df_test = pd.merge(df_transaction_test, df_identity_test, how = \"left\", on = \"TransactionID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of df_identity_train: \", df_identity_train.shape)\n",
    "print(\"shape of df_transaction_train: \", df_transaction_train.shape)\n",
    "print(\"shape of df_train: \", df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_identity_train, df_transaction_train, df_identity_test, df_transaction_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_test.columns:\n",
    "    if df_test[col].dtype != 'object':\n",
    "        print(col, df_train[col].min(), df_test[col].min(), df_train[col].max(), df_test[col].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_val_cols =      [col for col in df_train.columns if df_train[col].nunique()<=1] +\\\n",
    "                        [col for col in df_test.columns if df_test[col].nunique()<=1]\n",
    "missing_val_cols =  [col for col in df_train.columns if df_train[col].isnull().sum()/df_train.shape[0]>0.9] +\\\n",
    "                        [col for col in df_test.columns if df_test[col].isnull().sum()/df_test.shape[0]>0.9]\n",
    "same_val_cols =     [col for col in df_train.columns if df_train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9] +\\\n",
    "                        [col for col in df_test.columns if df_test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n",
    "cols_to_drop = list(set(one_val_cols + missing_val_cols + same_val_cols))\n",
    "print(cols_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop.remove('isFraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train[\"id_30\"].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(cols_to_drop, axis = 1, inplace = True)\n",
    "df_test.drop(cols_to_drop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.fillna(-999, inplace = True)\n",
    "df_test.fillna(-999, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumetable(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_day_feature(df, offset=0.58, tname='TransactionDT'):\n",
    "    \"\"\"\n",
    "    Creates a day of the week feature, encoded as 0-6.\n",
    "    \"\"\"\n",
    "    days = df[tname] / (3600 * 24)\n",
    "    encoded_days = np.floor(days - 1 + offset) % 7\n",
    "    return encoded_days\n",
    "\n",
    "def make_hour_feature(df, tname='TransactionDT'):\n",
    "    \"\"\"\n",
    "    Creates an hour of the day feature, encoded as 0-23.\n",
    "    \"\"\"\n",
    "    hours = df[tname] / (3600)\n",
    "    encoded_hours = np.floor(hours) % 24\n",
    "    return encoded_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Weekday\"] = make_day_feature(df_train)\n",
    "df_train[\"Hour\"]= make_hour_feature(df_train)\n",
    "df_test[\"Weekday\"] = make_day_feature(df_test)\n",
    "df_test[\"Hour\"] = make_hour_feature(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', \n",
    "          'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', \n",
    "          'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n",
    "          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', \n",
    "          'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', \n",
    "          'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other',\n",
    "          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', \n",
    "          'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', \n",
    "          'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n",
    "          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n",
    "          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', \n",
    "          'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', \n",
    "          'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', \n",
    "          'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', \n",
    "          'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple',\n",
    "          -999:\"undefined\"}\n",
    "us_emails = ['gmail', 'net', 'edu']\n",
    "# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499#latest_df-579654\n",
    "for col in ['P_emaildomain', 'R_emaildomain']:\n",
    "    df_train[col + '_pre'] = df_train[col].map(emails)\n",
    "    df_test[col + '_pre'] = df_test[col].map(emails)\n",
    "    \n",
    "    df_train[col + '_suffix'] = df_train[col].map(lambda x: str(x).split('.')[-1])\n",
    "    df_test[col + '_suffix'] = df_test[col].map(lambda x: str(x).split('.')[-1])\n",
    "    \n",
    "    df_train[col + '_suffix'] = df_train[col + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "    df_test[col + '_suffix'] = df_test[col + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"P_emaildomain_suffix\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_train.columns:\n",
    "    if col.startswith(\"id\"):\n",
    "        print(col, df_train[col].nunique())\n",
    "        if df_train[col].dtype!='object':\n",
    "            print(df_train[col].nunique(), sorted(df_train[col].unique())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = [\"ProductCD\", \"card1\", \"card2\", \"card3\", \"card4\", \"card5\", \"card6\", \"addr1\", \"addr2\",\n",
    "               \"P_emaildomain\", \"R_emaildomain\"] +  [\"M\" + str(i) for i in range(1, 10)] +\\\n",
    "                    ['DeviceType', 'DeviceInfo', 'Weekday', 'Hour',\n",
    "                     'P_emaildomain_pre', 'P_emaildomain_suffix', 'R_emaildomain_pre',\n",
    "                     'R_emaildomain_suffix', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_19', 'id_20', \n",
    "                     'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "category_counts = {}\n",
    "for col in categorical:\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(df_train[col].values) + list(df_test[col].values))\n",
    "    df_train[col] = lbl.transform(list(df_train[col].values))\n",
    "    df_test[col] = lbl.transform(list(df_test[col].values))\n",
    "    category_counts[col] = len(list(lbl.classes_)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical:\n",
    "    print(col, df_train[col].nunique(), df_train[col].min(), df_train[col].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.columns[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train[\"TransactionID\"]\n",
    "del df_train[\"TransactionDT\"]\n",
    "del df_test[\"TransactionID\"]\n",
    "del df_test[\"TransactionDT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = reduce_mem_usage(df_train)\n",
    "df_test = reduce_mem_usage(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train['isFraud']\n",
    "x_train = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(np.concatenate([x_train, x_test]))\n",
    "scaled_x_train = scaler.transform(x_train)\n",
    "scaled_x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_cv, Y_train, Y_cv = train_test_split(scaled_x_train, y_train, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_cv.shape)\n",
    "print(scaled_x_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaled_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/X_train_v5.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(X_train, handle)\n",
    "with open(\"../Data/X_cv_v5.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(X_cv, handle)\n",
    "with open(\"../Data/X_test_v5.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(X_test, handle)\n",
    "with open(\"../Data/Y_train_v5.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(Y_train, handle)\n",
    "with open(\"../Data/Y_cv_v5.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(Y_cv, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learnt from https://www.kaggle.com/pavelvpster/ieee-fraud-eda-lightgbm-baseline/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(data=X_train.astype('float32'), label=Y_train.astype('float32'))\n",
    "lgb_valid = lgb.Dataset(data=X_cv.astype('float32'), label=Y_cv.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_leaves, min_data_in_leaf, max_depth, bagging_fraction, feature_fraction, lambda_l1, lambda_l2):\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'is_unbalance': False,\n",
    "        'boost_from_average': True,\n",
    "        'num_threads': 4,\n",
    "        \n",
    "        'num_leaves': int(num_leaves),\n",
    "        'min_data_in_leaf': int(min_data_in_leaf),\n",
    "        'max_depth': int(max_depth),\n",
    "        'bagging_fraction' : bagging_fraction,\n",
    "        'feature_fraction' : feature_fraction,\n",
    "        'lambda_l1': lambda_l1,\n",
    "        'lambda_l2': lambda_l2\n",
    "    }\n",
    "    \n",
    "    lgb_model = lgb.train(params, lgb_train, valid_sets=lgb_valid, verbose_eval=1000)\n",
    "    \n",
    "    y = lgb_model.predict(X_cv.astype('float32'), num_iteration=lgb_model.best_iteration)\n",
    "    \n",
    "    score = roc_auc_score(Y_cv.astype('float32'), y)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = {\n",
    "    'num_leaves': (31, 500),\n",
    "    'min_data_in_leaf': (20, 200),\n",
    "    'max_depth':(-1, 50),\n",
    "    'bagging_fraction' : (0.1, 0.9),\n",
    "    'feature_fraction' : (0.1, 0.9),\n",
    "    'lambda_l1': (0, 2),\n",
    "    'lambda_l2': (0, 2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo = BayesianOptimization(train_model, bounds, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo.maximize(init_points=10, n_iter=15, acq='ucb', xi=0.0, alpha=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'is_unbalance': False,\n",
    "    'boost_from_average': True,\n",
    "    'num_threads': 4,\n",
    "    \n",
    "    'num_leaves': int(bo.max['params']['num_leaves']),\n",
    "    'min_data_in_leaf': int(bo.max['params']['min_data_in_leaf']),\n",
    "    'max_depth': int(bo.max['params']['max_depth']),\n",
    "    'bagging_fraction' : bo.max['params']['bagging_fraction'],\n",
    "    'feature_fraction' : bo.max['params']['feature_fraction'],\n",
    "    'lambda_l1': bo.max['params']['lambda_l1'],\n",
    "    'lambda_l2': bo.max['params']['lambda_l2']\n",
    "}\n",
    "\n",
    "lgb_model = lgb.train(params, lgb_train, valid_sets=lgb_valid, verbose_eval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = lgb_model.predict(X_test.astype('float32'), num_iteration=lgb_model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../Data/sample_submission.csv', index_col='TransactionID')\n",
    "submission['isFraud'] = Y_pred\n",
    "submission.to_csv('../Data/Y_test_v5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
